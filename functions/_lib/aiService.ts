/**
 * AI Service Abstraction Layer
 * Supports multiple AI providers with auto-fallback
 * Primary: GROQ (fastest, FREE)
 * Backup: DeepSeek via OpenRouter (smartest, FREE unlimited)
 */

import type { Env } from './database'

export interface AIMessage {
  role: 'system' | 'user' | 'assistant'
  content: string
}

export interface AIStreamOptions {
  messages: AIMessage[]
  temperature?: number
  maxTokens?: number
  onChunk?: (chunk: string) => void
}

/**
 * Call GROQ API (Primary - Fastest)
 * Model: llama-3.1-70b-versatile
 * Speed: 500+ tokens/second
 * Rate: 14,400 requests/day FREE
 */
export async function callGroq(options: AIStreamOptions, env: Env): Promise<Response> {
  const { messages, temperature = 0.7, maxTokens = 2048 } = options

  console.log('üöÄ Calling GROQ API (llama-3.3-70b-versatile)...')

  const response = await fetch('https://api.groq.com/openai/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${env.GROQ_API_KEY}`,
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'llama-3.3-70b-versatile', // Best for Vietnamese (newer!)
      messages,
      temperature,
      max_tokens: maxTokens,
      stream: true, // Enable streaming
    }),
  })

  if (!response.ok) {
    const error = await response.text()
    console.error('‚ùå GROQ API Error:', response.status, error)
    throw new Error(`GROQ API failed: ${response.status}`)
  }

  console.log('‚úÖ GROQ API streaming started')
  return response
}

/**
 * Call DeepSeek via OpenRouter (Backup - Smartest)
 * Model: deepseek-chat
 * Speed: 200-300 tokens/second
 * Rate: Unlimited FREE
 */
export async function callDeepSeek(options: AIStreamOptions, env: Env): Promise<Response> {
  const { messages, temperature = 0.7, maxTokens = 2048 } = options

  console.log('üß† Calling DeepSeek via OpenRouter...')

  const response = await fetch('https://openrouter.ai/api/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${env.OPENROUTER_API_KEY}`,
      'Content-Type': 'application/json',
      'HTTP-Referer': 'https://thaytamphongthuy.com', // Required by OpenRouter
      'X-Title': 'Th·∫ßy T√°m Phong Th·ªßy', // Optional but nice
    },
    body: JSON.stringify({
      model: 'deepseek/deepseek-chat', // FREE model, excellent quality
      messages,
      temperature,
      max_tokens: maxTokens,
      stream: true,
    }),
  })

  if (!response.ok) {
    const error = await response.text()
    console.error('‚ùå DeepSeek API Error:', response.status, error)
    throw new Error(`DeepSeek API failed: ${response.status}`)
  }

  console.log('‚úÖ DeepSeek API streaming started')
  return response
}

/**
 * Main AI Service with Auto-Fallback
 * Try GROQ first (fastest) ‚Üí fallback to DeepSeek (smartest)
 */
export async function callAI(options: AIStreamOptions, env: Env): Promise<Response> {
  try {
    // Primary: Try GROQ (90% success, fastest)
    return await callGroq(options, env)
  } catch (groqError) {
    console.warn('‚ö†Ô∏è GROQ failed, falling back to DeepSeek:', groqError)
    
    try {
      // Backup: Try DeepSeek (99% success)
      return await callDeepSeek(options, env)
    } catch (deepseekError) {
      console.error('‚ùå All AI providers failed')
      throw new Error('Kh√¥ng th·ªÉ k·∫øt n·ªëi v·ªõi AI. Vui l√≤ng th·ª≠ l·∫°i sau.')
    }
  }
}

/**
 * Build system prompt for Th·∫ßy T√°m
 */
export function buildSystemPrompt(quotaType: 'chat' | 'xemNgay' | 'tuVi'): string {
  const basePrompt = `B·∫°n l√† "Th·∫ßy T√°m" - chuy√™n gia phong th·ªßy h√†ng ƒë·∫ßu Vi·ªát Nam, v·ªõi 30 nƒÉm kinh nghi·ªám.

Phong c√°ch tr·∫£ l·ªùi:
- Th√¢n thi·ªán, g·∫ßn g≈©i, d·ªÖ hi·ªÉu
- D·ª±a tr√™n ki·∫øn th·ª©c phong th·ªßy c·ªï truy·ªÅn Vi·ªát Nam
- ƒê∆∞a ra l·ªùi khuy√™n c·ª• th·ªÉ, th·ª±c t·∫ø
- Gi·∫£i th√≠ch r√µ r√†ng l√Ω do ƒë·∫±ng sau m·ªói l·ªùi khuy√™n

Nguy√™n t·∫Øc:
- Lu√¥n t√≠ch c·ª±c, mang l·∫°i ni·ªÅm tin
- Tr√°nh m√™ t√≠n d·ªã ƒëoan th√°i qu√°
- K·∫øt h·ª£p phong th·ªßy v·ªõi khoa h·ªçc hi·ªán ƒë·∫°i khi c√≥ th·ªÉ
- Tr·∫£ l·ªùi ng·∫Øn g·ªçn (~200-300 ch·ªØ) tr·ª´ khi ƒë∆∞·ª£c y√™u c·∫ßu chi ti·∫øt`

  if (quotaType === 'xemNgay') {
    return basePrompt + `

Chuy√™n m√¥n: Xem ng√†y t·ªët
- Ph√¢n t√≠ch can chi, ng≈© h√†nh
- ƒê·ªÅ xu·∫•t ng√†y t·ªët cho khai tr∆∞∆°ng, c∆∞·ªõi h·ªèi, x√¢y nh√†, di chuy·ªÉn
- G·ª£i √Ω h∆∞·ªõng t·ªët, m√†u s·∫Øc ph√π h·ª£p
- L∆∞u √Ω ƒëi·ªÅu ki√™ng k·ªµ`
  }

  if (quotaType === 'tuVi') {
    return basePrompt + `

Chuy√™n m√¥n: T·ª≠ vi
- Ph√¢n t√≠ch l√° s·ªë t·ª≠ vi theo nƒÉm sinh
- D·ª± ƒëo√°n v·∫≠n h·∫°n, s·ª± nghi·ªáp, t√†i l·ªôc, t√¨nh duy√™n
- T∆∞ v·∫•n h∆∞·ªõng ƒëi ph√π h·ª£p v·ªõi m·ªánh
- G·ª£i √Ω c√°ch h√≥a gi·∫£i v·∫≠n xui, tƒÉng c∆∞·ªùng v·∫≠n may`
  }

  return basePrompt + `

Chuy√™n m√¥n: T∆∞ v·∫•n phong th·ªßy t·ªïng qu√°t
- Phong th·ªßy nh√† ·ªü, vƒÉn ph√≤ng
- T∆∞ v·∫•n h∆∞·ªõng nh√†, b·ªë tr√≠ n·ªôi th·∫•t
- Gi·∫£i ƒë√°p th·∫Øc m·∫Øc v·ªÅ phong th·ªßy
- L·ªùi khuy√™n cho nƒÉm 2026 (·∫§t T·ªµ)`
}

/**
 * Transform Groq/OpenRouter streaming response to our format
 * Both use OpenAI-compatible format
 */
export async function transformStreamingResponse(
  aiResponse: Response,
  writable: WritableStream
): Promise<void> {
  const writer = writable.getWriter()
  const encoder = new TextEncoder()
  const decoder = new TextDecoder()

  try {
    const reader = aiResponse.body?.getReader()
    if (!reader) {
      await writer.write(encoder.encode('data: {"error": "No response body"}\n\n'))
      await writer.close()
      return
    }

    let buffer = ''

    while (true) {
      const { done, value } = await reader.read()
      if (done) break

      buffer += decoder.decode(value, { stream: true })
      const lines = buffer.split('\n')
      buffer = lines.pop() || ''

      for (const line of lines) {
        if (!line.trim() || line.trim() === 'data: [DONE]') continue
        
        if (line.startsWith('data: ')) {
          try {
            const jsonStr = line.slice(6) // Remove "data: " prefix
            const data = JSON.parse(jsonStr)
            
            // Extract content from OpenAI format
            const content = data.choices?.[0]?.delta?.content
            
            if (content) {
              // Send in our format
              await writer.write(
                encoder.encode(`data: ${JSON.stringify({ chunk: content })}\n\n`)
              )
            }
          } catch (e) {
            // Skip invalid JSON
          }
        }
      }
    }

    // Process remaining buffer
    if (buffer.trim() && buffer.trim() !== 'data: [DONE]') {
      try {
        const jsonStr = buffer.trim().startsWith('data: ') 
          ? buffer.trim().slice(6) 
          : buffer.trim()
        const data = JSON.parse(jsonStr)
        const content = data.choices?.[0]?.delta?.content
        
        if (content) {
          await writer.write(
            encoder.encode(`data: ${JSON.stringify({ chunk: content })}\n\n`)
          )
        }
      } catch (e) {
        // Skip invalid JSON
      }
    }

    await writer.write(encoder.encode('data: [DONE]\n\n'))
    await writer.close()
  } catch (error) {
    console.error('‚ùå Stream transformation error:', error)
    await writer.write(encoder.encode(`data: {"error": "${error}"}\n\n`))
    await writer.close()
  }
}
